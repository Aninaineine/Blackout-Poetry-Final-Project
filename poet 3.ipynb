{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3a7b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/ethanyattaw/opt/anaconda3/lib/python3.9/site-packages (3.7)\r\n",
      "Requirement already satisfied: tqdm in /Users/ethanyattaw/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.0)\r\n",
      "Requirement already satisfied: joblib in /Users/ethanyattaw/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\r\n",
      "Requirement already satisfied: click in /Users/ethanyattaw/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ethanyattaw/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.3.15)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ethanyattaw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ethanyattaw/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ethanyattaw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "# for reference:\n",
    "acronym_meanings = {\n",
    "    'CC': 'Coordinating conjunction',\n",
    "    'CD': 'Cardinal number',\n",
    "    'DT': 'Determiner',\n",
    "    'EX': 'Existential there',\n",
    "    'FW': 'Foreign word',\n",
    "    'IN': 'Preposition or subordinating conjunction',\n",
    "    'JJ*': 'Adjective',\n",
    "    'MD': 'Modal',\n",
    "    'NN*': 'Noun',\n",
    "    'PDT': 'Predeterminer',\n",
    "    'POS': 'Possessive ending',\n",
    "    'PRP*': 'Pronoun',\n",
    "    'RB*': 'Adverb',\n",
    "    'RP': 'Particle',\n",
    "    'SYM': 'Symbol',\n",
    "    'TO': 'To',\n",
    "    'UH': 'Interjection',\n",
    "    'VB*': 'Verb',\n",
    "    'WDT': 'Wh-determiner',\n",
    "    'WP*': 'Wh-pronoun',\n",
    "    'WRB': 'Wh-adverb'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7d1193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a topic to make blackout poetry out of! coffee\n",
      "\n",
      "['world', 'prepared', 'drinking', 'were', 'Middle', 'cultivated', 'sales']\n",
      "[41, 106, 182, 205, 245, 293, 352]\n",
      "['NN', 'JJ', 'NN', 'VB', 'NN', 'VB', 'NN']\n",
      "[(216, 220), (569, 576), (952, 959), (1086, 1089), (1305, 1310), (1565, 1574), (1871, 1875)]\n",
      "\n",
      "pops=0\n",
      "\n",
      "Coffee is a beverage prepared from roasted coffee beans. Darkly colored, bitter, and slightly acidic, coffee has a stimulating effect on humans, primarily due to its caffeine content. It has the highest sales in the world market for hot drinks.Seeds of the Coffea plant's fruits are separated to produce unroasted green coffee beans. The beans are roasted and then ground into fine particles that are typically steeped in hot water before being filtered out, producing a cup of coffee. It is usually served hot, although chilled or iced coffee is common. Coffee can be prepared and presented in a variety of ways (e.g., espresso, French press, caff√® latte, or already-brewed canned coffee). Sugar, sugar substitutes, milk, and cream are often used to mask the bitter taste or enhance the flavor. \n",
      "Though coffee is now a global commodity, it has a long history tied closely to food traditions around the Red Sea. The earliest credible evidence of coffee drinking in the form of the modern beverage appears in modern-day Yemen from the mid-15th century in Sufi shrines, where coffee seeds were first roasted and brewed in a manner similar to current methods. The Yemenis procured the coffee beans from the Ethiopian Highlands and began cultivation. By the 16th century, the drink had reached the rest of the Middle East and North Africa, later spreading to Europe. In the 20th century, coffee became a global commodity, creating different coffee cultures around the world. \n",
      "The two most commonly grown coffee bean types are C. arabica and C. robusta. Coffee plants are cultivated in over 70 countries, primarily in the equatorial regions of the Americas, Southeast Asia, the Indian subcontinent, and Africa. As of 2018, Brazil was the leading grower of coffee beans, producing 35% of the world's total. Green, unroasted coffee is traded as an agricultural commodity. Despite sales of coffee reaching billions of dollars worldwide, farmers producing coffee beans disproportionately live in poverty. Critics of the coffee industry have also pointed to its negative impact on the environment and the clearing of land for coffee-growing and water use. \n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def search_wiki():\n",
    "        global text\n",
    "        user_input = input(\"Choose a topic to make blackout poetry out of! \").lower()\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                text = wiki_pull(user_input)\n",
    "                if text == \"\":\n",
    "                    text = wiki_pull_backup(user_input)\n",
    "                    print(f\"Your search terms weren't a perfect match. Here's the closest article we found: {closest_match}\\n\")\n",
    "\n",
    "            except:\n",
    "                text = wiki_pull_backup(user_input)\n",
    "                print(f\"Your search terms weren't a perfect match. Here's the closest article we found: {closest_match}\\n\")\n",
    "        except:\n",
    "            print(f\"\\nYour search terms didn't match any results! Please try again\")\n",
    "            search_wiki()\n",
    "\n",
    "\n",
    "def wiki_pull(user_input):\n",
    "    wiki_api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # parameters\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": \"\", # get intro section only\n",
    "        \"explaintext\": \"\", # plain text\n",
    "        \"titles\": user_input}\n",
    "\n",
    "    # making request\n",
    "    response = requests.get(wiki_api_url, params=params)\n",
    "    response_json = response.json()\n",
    "\n",
    "    # have to get page_id so we can index the json dictionary for the extract\n",
    "    page_id = list(response_json[\"query\"][\"pages\"].keys())[0]\n",
    "    page_content = response_json[\"query\"][\"pages\"][page_id][\"extract\"]\n",
    "\n",
    "    return page_content\n",
    "\n",
    "\n",
    "\n",
    "def wiki_pull_backup(user_topic):\n",
    "    \n",
    "    wiki_api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # using search action instead of query\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": user_topic,\n",
    "        \"srprop\": \"\",\n",
    "        \"utf8\": \"\"}\n",
    "\n",
    "    # making request\n",
    "    response = requests.get(wiki_api_url, params=params)\n",
    "    response_json = response.json()\n",
    "\n",
    "    # selecting the page with closest match\n",
    "    global closest_match\n",
    "    closest_match = response_json[\"query\"][\"search\"][0][\"title\"]\n",
    "\n",
    "    # using the selected title for content request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": \"\", # get intro section only\n",
    "        \"explaintext\": \"\", # plain text\n",
    "        \"titles\": closest_match}\n",
    "\n",
    "    # making request for content\n",
    "    response = requests.get(wiki_api_url, params=params)\n",
    "    response_json = response.json()\n",
    "\n",
    "    # have to get page_id so we can index the json dictionary for the extract\n",
    "    page_id = list(response_json[\"query\"][\"pages\"].keys())[0]\n",
    "    page_content = response_json[\"query\"][\"pages\"][page_id][\"extract\"]\n",
    "\n",
    "    return page_content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the parts of speech that are allowed in the output\n",
    "allowed_pos = [\"JJ\", \"RB\", \"VB\", \"NN\"]\n",
    "\n",
    "\n",
    "result_tuples = []\n",
    "chosen = []\n",
    "\n",
    "# pops is used to limit the number of loops. Sometimes it bugs out if max_pops is set too high.\n",
    "max_pops = 25\n",
    "global pops\n",
    "pops = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def text2tokens(text):\n",
    "    global tokens, text_copy, text_offset\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # to preserve a version of the original text\n",
    "    text_copy = text\n",
    "    \n",
    "    # fixing \\n offset\n",
    "    text_offset = text.replace(\"\\n\", \" \\n\")\n",
    "    \n",
    "    # attaching the index from the tokenized text AND the original text to each word (both are later added to result_tuple)\n",
    "    # note: i think \\n's in the text source offset the text_index by 1 character\n",
    "    global both_indices\n",
    "    both_indices = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        text_index = text_copy.find(token) # searches for substring in string\n",
    "        text_index_end = text_index + (len(token)-1)\n",
    "        text_copy = text_copy.replace(token, \"x\" * len(token), 1) # replace the current token with all x's\n",
    "        both_indices.append((token, i, text_index, text_index_end))\n",
    "    \n",
    "    \n",
    "\n",
    "# not sure if stopwords are good or bad--a lot are already avoided due to \"allowed_pos\"\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     bad_tokens = []\n",
    "#     for word in bad_tokens:\n",
    "#         if word.lower() not in stop_words:\n",
    "#             bad_tokens.append(word)\n",
    "\n",
    "\n",
    "\n",
    "def get_word():\n",
    "    global choice, new_word, new_word_index, new_word_pos\n",
    "    choice = random.choice(range(len(tokens)))\n",
    "    chosen.append(choice)\n",
    "    new_word = tokens[choice]\n",
    "    new_word_index = choice\n",
    "    new_word_pos = pos_tag([new_word])[0][1][:2] # syntax: pos_tag generates a list of a tuple. This grabs the tuple, then grabs the 2nd element which is the POS as a string, then grabs the first 2 letters.\n",
    "    \n",
    "    \n",
    "    \n",
    "# this function is generalizable to any POS sequences i believe\n",
    "def sequence_check():\n",
    "    \"\"\"Checks to see if there is an sequence like 'adjective then noun' around the chosen word. \n",
    "    If so, then it adds both words to the results list.\"\"\"\n",
    "    \n",
    "    global new_word, new_word_index, new_word_pos\n",
    "    if choice > 0 and choice < len(tokens) - 1: # avoids list index out of range errors\n",
    "        prior_word = tokens[choice-1]\n",
    "        prior_word_index = choice-1\n",
    "        prior_word_pos = pos_tag([prior_word])[0][1][:2]\n",
    "\n",
    "        next_word = tokens[choice+1]\n",
    "        next_word_index = choice+1\n",
    "        next_word_pos = pos_tag([next_word])[0][1][:2]\n",
    "    \n",
    "    # adj-noun-verb\n",
    "        if prior_word_pos == \"JJ\" and new_word_pos == \"NN\" and next_word_pos ==\"VB\":\n",
    "            new_word = prior_word\n",
    "            new_word_index = prior_word_index\n",
    "            new_word_pos = prior_word_pos\n",
    "            check_and_insert()\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "            print(\"adj-noun-verb worked :)\")\n",
    "    \n",
    "    # adj-noun\n",
    "        elif prior_word_pos == \"JJ\" and new_word_pos == \"NN\":\n",
    "            new_word = prior_word\n",
    "            new_word_index = prior_word_index\n",
    "            new_word_pos = prior_word_pos\n",
    "            check_and_insert()\n",
    "            print(\"adj-noun worked :)\")\n",
    "\n",
    "        elif new_word_pos == \"JJ\" and next_word_pos == \"NN\":\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "            print(\"adj-noun worked :)\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def check_and_insert():\n",
    "    # Criteria:\n",
    "\n",
    "    # regular expression that matches only word characters. Prevents stuff like \"]\" which is a noun for some reason.\n",
    "    pattern = re.compile(r'\\w+')\n",
    "    if pattern.match(new_word):\n",
    "    \n",
    "    \n",
    "    \n",
    "        if new_word_pos in allowed_pos:\n",
    "\n",
    "            # find the index to insert the new word at.\n",
    "            index_to_insert = len(result_tuples)\n",
    "            for i, (word, index, pos, text_index, text_index_end) in enumerate(result_tuples): # this uses tuple unpacking and the enumerate function\n",
    "                if new_word_index < index:\n",
    "                    index_to_insert = i\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "            # check if the new_word_pos is different from the POS of the words before and after the insertion point. checks 2 things:\n",
    "                # 1. does the word before the insertion point have a different POS, or is the new word gonna be the first word in the list.\n",
    "                # 2. does the word after the insertion point have a different POS, or is the new word gonna be the last word in the list.\n",
    "            if (index_to_insert == 0 or result_tuples[index_to_insert-1][2] != new_word_pos) and (index_to_insert == len(result_tuples) or result_tuples[index_to_insert][2] != new_word_pos):\n",
    "                \n",
    "                # getting index from original text\n",
    "                for (token, token_index, text_index, text_index_end) in both_indices:\n",
    "                    if new_word_index == token_index:\n",
    "                        new_text_index = text_index\n",
    "                        new_text_index_end = text_index_end\n",
    "                \n",
    "                # insert the new word's tuple into the result_tuples list\n",
    "                result_tuples.insert(index_to_insert, (new_word, new_word_index, new_word_pos, new_text_index, new_text_index_end))\n",
    "\n",
    "\n",
    "        if len(result_tuples) == 7:\n",
    "\n",
    "    #         # force the last word to be a noun--not sure if this helps or not\n",
    "    #         last_word_pos = result_tuples[-1][2][:2]\n",
    "    #         if last_word_pos != \"NN\":\n",
    "    #             result_tuples.pop()\n",
    "    #             global pops\n",
    "    #             pops +=1\n",
    "\n",
    "            # force the first word to not be a verb\n",
    "            first_word_pos = result_tuples[0][2][:2]\n",
    "            if first_word_pos == \"VB\":\n",
    "                result_tuples.pop()\n",
    "                global pops\n",
    "                pops +=1\n",
    "    \n",
    "\n",
    "while_counter = 0    \n",
    "def main():\n",
    "    search_wiki()\n",
    "    \n",
    "    text2tokens(text)\n",
    "    \n",
    "    while_counter = 0\n",
    "    while len(result_tuples) < 7 and pops < max_pops and while_counter < 150:\n",
    "        get_word()\n",
    "        check_and_insert()\n",
    "        sequence_check()\n",
    "        \n",
    "        while_counter +=1\n",
    "    \n",
    "    \n",
    "main()\n",
    "    \n",
    "\n",
    "result_words = []\n",
    "result_index = [] # result_index and result_pos lists are for easy reference to ensure the criteria worked\n",
    "result_pos = []\n",
    "result_text_index = []\n",
    "for (word, index, pos, text_index, text_index_end) in result_tuples:\n",
    "    result_words.append(word)\n",
    "    result_index.append(index)\n",
    "    result_pos.append(pos)\n",
    "    result_text_index.append((text_index, text_index_end))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n{result_words}\\n{result_index}\\n{result_pos}\\n{result_text_index}\\n\\npops={pops}\\n\\n{text_offset}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ac4db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "print(text[216])\n",
    "print(text[220])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
