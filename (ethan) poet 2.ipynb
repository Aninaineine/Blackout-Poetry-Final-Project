{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fef9aee",
   "metadata": {},
   "source": [
    "Sources\n",
    "1. unpacking tuples: https://www.w3schools.com/python/python_tuples_unpack.asp\n",
    "2. enumerate: https://www.geeksforgeeks.org/enumerate-in-python/#\n",
    "3. reg ex: https://stackoverflow.com/questions/50888683/meaning-of-re-compiler-w-in-python\n",
    "4. lambda: https://www.w3schools.com/python/python_lambda.asp\n",
    "5. sorted: https://www.w3schools.com/python/ref_func_sorted.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bd6e7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/ethanyattaw/opt/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /Users/ethanyattaw/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/ethanyattaw/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ethanyattaw/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in /Users/ethanyattaw/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ethanyattaw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ethanyattaw/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ethanyattaw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "# for reference:\n",
    "acronym_meanings = {\n",
    "    'CC': 'Coordinating conjunction',\n",
    "    'CD': 'Cardinal number',\n",
    "    'DT': 'Determiner',\n",
    "    'EX': 'Existential there',\n",
    "    'FW': 'Foreign word',\n",
    "    'IN': 'Preposition or subordinating conjunction',\n",
    "    'JJ*': 'Adjective',\n",
    "    'MD': 'Modal',\n",
    "    'NN*': 'Noun',\n",
    "    'PDT': 'Predeterminer',\n",
    "    'POS': 'Possessive ending',\n",
    "    'PRP*': 'Pronoun',\n",
    "    'RB*': 'Adverb',\n",
    "    'RP': 'Particle',\n",
    "    'SYM': 'Symbol',\n",
    "    'TO': 'To',\n",
    "    'UH': 'Interjection',\n",
    "    'VB*': 'Verb',\n",
    "    'WDT': 'Wh-determiner',\n",
    "    'WP*': 'Wh-pronoun',\n",
    "    'WRB': 'Wh-adverb'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdda128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # potentially useful things:\n",
    "\n",
    "# getting POS of all words\n",
    "pos_tuple = pos_tag(tokens)\n",
    "\n",
    "# turning tuple into a dictionary--does not allow for repeats so not useful to serve as the index for the final output\n",
    "pos_dict = {}\n",
    "for word, pos in pos_tuple: #tuple unpacking\n",
    "    pos_dict[word] = pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee1461",
   "metadata": {},
   "source": [
    "Forcing adjacent words to have different POS's makes the output much more intelligble. It also prevents the same word from appearing twice in a row.\n",
    "# Program Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffdcd1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj-noun worked :)\n",
      "adj-noun worked :)\n",
      "\n",
      "['freestones', 'are', 'varies', 'white-fleshed', 'peaches', 'are', 'other', 'fruit']\n",
      "[7, 32, 52, 64, 65, 66, 99, 100]\n",
      "['NN', 'VB', 'NN', 'JJ', 'NN', 'VB', 'JJ', 'NN']\n",
      "\n",
      "pops=0\n"
     ]
    }
   ],
   "source": [
    "text = \"Cultivated peaches are divided into clingstones and freestones, depending on whether the flesh sticks to the stone or not; both can have either white or yellow flesh. Peaches with white flesh typically are very sweet with little acidity, while yellow-fleshed peaches typically have an acidic tang coupled with sweetness, though this also varies greatly. Both colors often have some red on their skins. Low-acid, white-fleshed peaches are the most popular kinds in China, Japan, and neighbouring Asian countries, while Europeans and North Americans have historically favoured the acidic, yellow-fleshed cultivars. Peach trees are relatively short-lived as compared with some other fruit trees. In some regions orchards are replanted after 8 to 10 years, while in others trees may produce satisfactorily for 20 to 25 years or more, depending upon their resistance to diseases, pests, and winter damage.\"\n",
    "text2 = \"A wave of Confederate surrenders followed. On April 14, just five days after Lee's surrender, Lincoln was assassinated. As a practical matter, the war ended with the May 26 surrender of the Department of the Trans-Mississippi but the conclusion of the American Civil War lacks a clear and precise historical end date. Confederate ground forces continued surrendering past the May 26 surrender date until June 23. By the end of the war, much of the South's infrastructure was destroyed, especially its railroads. The Confederacy collapsed, slavery was abolished, and four million enslaved black people were freed. The war-torn nation then entered the Reconstruction era in an attempt to rebuild the country, bring the former Confederate states back into the United States, and grant civil rights to freed slaves.\"\n",
    "text3 = \"In October 2006, YouTube was bought by Google for $1.65 billion.[11] Google's ownership of YouTube expanded the site's business model, expanding from generating revenue from advertisements alone to offering paid content such as movies and exclusive content produced by YouTube. It also offers YouTube Premium, a paid subscription option for watching content without ads. YouTube also approved creators to participate in Google's AdSense program, which seeks to generate more revenue for both parties. YouTube reported revenue of $29.2 billion in 2022.[12] In 2021, YouTube's annual advertising revenue increased to $28.8 billion, an increase in revenue of 9 billion from the previous year.[1]\"\n",
    "# text = text3\n",
    "\n",
    "\n",
    "\n",
    "# the parts of speech that are allowed in the output\n",
    "allowed_pos = [\"JJ\", \"RB\", \"VB\", \"NN\"]\n",
    "\n",
    "\n",
    "result_tuples = []\n",
    "chosen = []\n",
    "\n",
    "# pops is used to limit the number of loops. Sometimes it bugs out if max_pops is set too high.\n",
    "max_pops = 25\n",
    "global pops\n",
    "pops = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def text2tokens(text):\n",
    "    bad_tokens = word_tokenize(text)\n",
    "\n",
    "# not sure if stopwords are good or bad--a lot are already avoided due to \"allowed_pos\"\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     bad_tokens = []\n",
    "#     for word in bad_tokens:\n",
    "#         if word.lower() not in stop_words:\n",
    "#             bad_tokens.append(word)\n",
    "\n",
    "    # regular expression that matches only word characters. Prevents stuff like \"]\" which is a noun for some reason.\n",
    "    pattern = re.compile(r'\\w+')\n",
    "    \n",
    "    global tokens\n",
    "    tokens = []\n",
    "    for token in bad_tokens:\n",
    "        if pattern.match(token):\n",
    "            tokens.append(token)\n",
    "\n",
    "\n",
    "def get_word():\n",
    "    global choice, new_word, new_word_index, new_word_pos\n",
    "    choice = random.choice(range(len(tokens)))\n",
    "    if choice in chosen: # prevents the same word from being used twice. (maybe obsolete now)\n",
    "        get_word()\n",
    "    chosen.append(choice)\n",
    "    new_word = tokens[choice]\n",
    "    new_word_index = choice\n",
    "    new_word_pos = pos_tag([new_word])[0][1][:2] # syntax: pos_tag generates a list of a tuple. This grabs the tuple, then grabs the 2nd element which is the POS as a string, then grabs the first 2 letters.\n",
    "    \n",
    "    \n",
    "    \n",
    "# this function is generalizable to any POS sequences i believe\n",
    "def sequence_check():\n",
    "    \"\"\"Checks to see if there is an sequence like 'adjective then noun' around the chosen word. \n",
    "    If so, then it adds both words to the results list.\"\"\"\n",
    "    \n",
    "    global new_word, new_word_index, new_word_pos\n",
    "    if choice > 0 and choice < len(tokens) - 1: # avoids list index out of range errors\n",
    "        prior_word = tokens[choice-1]\n",
    "        prior_word_index = choice-1\n",
    "        prior_word_pos = pos_tag([prior_word])[0][1][:2]\n",
    "\n",
    "        next_word = tokens[choice+1]\n",
    "        next_word_index = choice+1\n",
    "        next_word_pos = pos_tag([next_word])[0][1][:2]\n",
    "    \n",
    "    # adj-noun-verb\n",
    "        if prior_word_pos == \"JJ\" and new_word_pos == \"NN\" and next_word_pos ==\"VB\":\n",
    "            new_word = prior_word\n",
    "            new_word_index = prior_word_index\n",
    "            new_word_pos = prior_word_pos\n",
    "            check_and_insert()\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "            print(\"adj-noun-verb worked :)\")\n",
    "    \n",
    "    # adj-noun\n",
    "        elif prior_word_pos == \"JJ\" and new_word_pos == \"NN\":\n",
    "            new_word = prior_word\n",
    "            new_word_index = prior_word_index\n",
    "            new_word_pos = prior_word_pos\n",
    "            check_and_insert()\n",
    "            print(\"adj-noun worked :)\")\n",
    "\n",
    "        elif new_word_pos == \"JJ\" and next_word_pos == \"NN\":\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "            print(\"adj-noun worked :)\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def check_and_insert():\n",
    "    # Criteria:\n",
    "\n",
    "    if new_word_pos in allowed_pos:\n",
    "\n",
    "        # find the index to insert the new word at.\n",
    "        index_to_insert = len(result_tuples)\n",
    "        for i, (word, index, pos) in enumerate(result_tuples): # this uses tuple unpacking and the enumerate function\n",
    "            if new_word_index < index:\n",
    "                index_to_insert = i\n",
    "                break\n",
    "\n",
    "                \n",
    "                \n",
    "        # check if the new_word_pos is different from the POS of the words before and after the insertion point. checks 2 things:\n",
    "            # 1. does the word before the insertion point have a different POS, or is the new word gonna be the first word in the list.\n",
    "            # 2. does the word after the insertion point have a different POS, or is the new word gonna be the last word in the list.\n",
    "        if (index_to_insert == 0 or result_tuples[index_to_insert-1][2] != new_word_pos) and (index_to_insert == len(result_tuples) or result_tuples[index_to_insert][2] != new_word_pos):\n",
    "            # insert the new word's tuple into the result_tuples list\n",
    "            result_tuples.insert(index_to_insert, (new_word, new_word_index, new_word_pos))\n",
    "\n",
    "\n",
    "    if len(result_tuples) == 7:\n",
    "        \n",
    "#         # force the last word to be a noun--not sure if this helps or not\n",
    "#         last_word_pos = result_tuples[-1][2][:2]\n",
    "#         if last_word_pos != \"NN\":\n",
    "#             result_tuples.pop()\n",
    "#             global pops\n",
    "#             pops +=1\n",
    "\n",
    "        # force the first word to not be a verb\n",
    "        first_word_pos = result_tuples[0][2][:2]\n",
    "        if first_word_pos == \"VB\":\n",
    "            result_tuples.pop()\n",
    "            global pops\n",
    "            pops +=1\n",
    "    \n",
    "       \n",
    "\n",
    "text2tokens(text)\n",
    "while len(result_tuples) < 7 and pops < max_pops:\n",
    "    get_word()\n",
    "    check_and_insert()\n",
    "    sequence_check()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# print(result_tuples)\n",
    "\n",
    "result_words = []\n",
    "result_index = [] # result_index and result_pos lists are for easy reference to ensure the criteria worked\n",
    "result_pos = []\n",
    "for result in result_tuples:\n",
    "    result_words.append(result[0])\n",
    "    result_index.append(result[1])\n",
    "    result_pos.append(result[2])\n",
    "\n",
    "print(f\"\\n{result_words}\\n{result_index}\\n{result_pos}\\n\\npops={pops}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
