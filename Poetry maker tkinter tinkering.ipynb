{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d811d64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nycta-p6\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\nycta-p6\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nycta-p6\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\nycta-p6\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nycta-p6\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nycta-p6\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NYCTA-P6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\NYCTA-P6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NYCTA-P6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "# for reference:\n",
    "acronym_meanings = {\n",
    "    'CC': 'Coordinating conjunction',\n",
    "    'CD': 'Cardinal number',\n",
    "    'DT': 'Determiner',\n",
    "    'EX': 'Existential there',\n",
    "    'FW': 'Foreign word',\n",
    "    'IN': 'Preposition or subordinating conjunction',\n",
    "    'JJ*': 'Adjective',\n",
    "    'MD': 'Modal',\n",
    "    'NN*': 'Noun',\n",
    "    'PDT': 'Predeterminer',\n",
    "    'POS': 'Possessive ending',\n",
    "    'PRP*': 'Pronoun',\n",
    "    'RB*': 'Adverb',\n",
    "    'RP': 'Particle',\n",
    "    'SYM': 'Symbol',\n",
    "    'TO': 'To',\n",
    "    'UH': 'Interjection',\n",
    "    'VB*': 'Verb',\n",
    "    'WDT': 'Wh-determiner',\n",
    "    'WP*': 'Wh-pronoun',\n",
    "    'WRB': 'Wh-adverb'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d004f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj-noun worked :)\n",
      "adj-noun worked :)\n",
      "adj-noun worked :)\n",
      "adj-noun worked :)\n",
      "adj-noun worked :)\n",
      "adj-noun worked :)\n",
      "adj-noun worked :)\n",
      "adj-noun worked :)\n",
      "adj-noun worked :)\n",
      "\n",
      "['Cultivated', 'clingstones', 'white', 'flesh', 'typically', 'yellow-fleshed']\n",
      "[0, 5, 29, 30, 31, 39]\n",
      "['VB', 'NN', 'JJ', 'NN', 'RB', 'JJ']\n",
      "\n",
      "pops=25\n"
     ]
    }
   ],
   "source": [
    "text = \"Cultivated peaches are divided into clingstones and freestones, depending on whether the flesh sticks to the stone or not; both can have either white or yellow flesh. Peaches with white flesh typically are very sweet with little acidity, while yellow-fleshed peaches typically have an acidic tang coupled with sweetness, though this also varies greatly. Both colors often have some red on their skins. Low-acid, white-fleshed peaches are the most popular kinds in China, Japan, and neighbouring Asian countries, while Europeans and North Americans have historically favoured the acidic, yellow-fleshed cultivars. Peach trees are relatively short-lived as compared with some other fruit trees. In some regions orchards are replanted after 8 to 10 years, while in others trees may produce satisfactorily for 20 to 25 years or more, depending upon their resistance to diseases, pests, and winter damage.\"\n",
    "text2 = \"A wave of Confederate surrenders followed. On April 14, just five days after Lee's surrender, Lincoln was assassinated. As a practical matter, the war ended with the May 26 surrender of the Department of the Trans-Mississippi but the conclusion of the American Civil War lacks a clear and precise historical end date. Confederate ground forces continued surrendering past the May 26 surrender date until June 23. By the end of the war, much of the South's infrastructure was destroyed, especially its railroads. The Confederacy collapsed, slavery was abolished, and four million enslaved black people were freed. The war-torn nation then entered the Reconstruction era in an attempt to rebuild the country, bring the former Confederate states back into the United States, and grant civil rights to freed slaves.\"\n",
    "text3 = \"In October 2006, YouTube was bought by Google for $1.65 billion.[11] Google's ownership of YouTube expanded the site's business model, expanding from generating revenue from advertisements alone to offering paid content such as movies and exclusive content produced by YouTube. It also offers YouTube Premium, a paid subscription option for watching content without ads. YouTube also approved creators to participate in Google's AdSense program, which seeks to generate more revenue for both parties. YouTube reported revenue of $29.2 billion in 2022.[12] In 2021, YouTube's annual advertising revenue increased to $28.8 billion, an increase in revenue of 9 billion from the previous year.[1]\"\n",
    "# text = text3\n",
    "\n",
    "\n",
    "\n",
    "# the parts of speech that are allowed in the output\n",
    "allowed_pos = [\"JJ\", \"RB\", \"VB\", \"NN\"]\n",
    "\n",
    "\n",
    "result_tuples = []\n",
    "chosen = []\n",
    "\n",
    "# pops is used to limit the number of loops. Sometimes it bugs out if max_pops is set too high.\n",
    "max_pops = 25\n",
    "global pops\n",
    "pops = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def text2tokens(text):\n",
    "    bad_tokens = word_tokenize(text)\n",
    "\n",
    "# not sure if stopwords are good or bad--a lot are already avoided due to \"allowed_pos\"\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     bad_tokens = []\n",
    "#     for word in bad_tokens:\n",
    "#         if word.lower() not in stop_words:\n",
    "#             bad_tokens.append(word)\n",
    "\n",
    "    # regular expression that matches only word characters. Prevents stuff like \"]\" which is a noun for some reason.\n",
    "    pattern = re.compile(r'\\w+')\n",
    "    \n",
    "    global tokens\n",
    "    tokens = []\n",
    "    for token in bad_tokens:\n",
    "        if pattern.match(token):\n",
    "            tokens.append(token)\n",
    "\n",
    "\n",
    "def get_word():\n",
    "    global choice, new_word, new_word_index, new_word_pos\n",
    "    choice = random.choice(range(len(tokens)))\n",
    "    if choice in chosen: # prevents the same word from being used twice. (maybe obsolete now)\n",
    "        get_word()\n",
    "    chosen.append(choice)\n",
    "    new_word = tokens[choice]\n",
    "    new_word_index = choice\n",
    "    new_word_pos = pos_tag([new_word])[0][1][:2] # syntax: pos_tag generates a list of a tuple. This grabs the tuple, then grabs the 2nd element which is the POS as a string, then grabs the first 2 letters.\n",
    "    \n",
    "    \n",
    "    \n",
    "# this function is generalizable to any POS sequences i believe\n",
    "def sequence_check():\n",
    "    \"\"\"Checks to see if there is an sequence like 'adjective then noun' around the chosen word. \n",
    "    If so, then it adds both words to the results list.\"\"\"\n",
    "    \n",
    "    global new_word, new_word_index, new_word_pos\n",
    "    if choice > 0 and choice < len(tokens) - 1: # avoids list index out of range errors\n",
    "        prior_word = tokens[choice-1]\n",
    "        prior_word_index = choice-1\n",
    "        prior_word_pos = pos_tag([prior_word])[0][1][:2]\n",
    "\n",
    "        next_word = tokens[choice+1]\n",
    "        next_word_index = choice+1\n",
    "        next_word_pos = pos_tag([next_word])[0][1][:2]\n",
    "    \n",
    "    # adj-noun-verb\n",
    "        if prior_word_pos == \"JJ\" and new_word_pos == \"NN\" and next_word_pos ==\"VB\":\n",
    "            new_word = prior_word\n",
    "            new_word_index = prior_word_index\n",
    "            new_word_pos = prior_word_pos\n",
    "            check_and_insert()\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "            print(\"adj-noun-verb worked :)\")\n",
    "    \n",
    "    # adj-noun\n",
    "        elif prior_word_pos == \"JJ\" and new_word_pos == \"NN\":\n",
    "            new_word = prior_word\n",
    "            new_word_index = prior_word_index\n",
    "            new_word_pos = prior_word_pos\n",
    "            check_and_insert()\n",
    "            print(\"adj-noun worked :)\")\n",
    "\n",
    "        elif new_word_pos == \"JJ\" and next_word_pos == \"NN\":\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "            print(\"adj-noun worked :)\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def check_and_insert():\n",
    "    # Criteria:\n",
    "\n",
    "    if new_word_pos in allowed_pos:\n",
    "\n",
    "        # find the index to insert the new word at.\n",
    "        index_to_insert = len(result_tuples)\n",
    "        for i, (word, index, pos) in enumerate(result_tuples): # this uses tuple unpacking and the enumerate function\n",
    "            if new_word_index < index:\n",
    "                index_to_insert = i\n",
    "                break\n",
    "\n",
    "                \n",
    "                \n",
    "        # check if the new_word_pos is different from the POS of the words before and after the insertion point. checks 2 things:\n",
    "            # 1. does the word before the insertion point have a different POS, or is the new word gonna be the first word in the list.\n",
    "            # 2. does the word after the insertion point have a different POS, or is the new word gonna be the last word in the list.\n",
    "        if (index_to_insert == 0 or result_tuples[index_to_insert-1][2] != new_word_pos) and (index_to_insert == len(result_tuples) or result_tuples[index_to_insert][2] != new_word_pos):\n",
    "            # insert the new word's tuple into the result_tuples list\n",
    "            result_tuples.insert(index_to_insert, (new_word, new_word_index, new_word_pos))\n",
    "\n",
    "\n",
    "    if len(result_tuples) == 7:\n",
    "        \n",
    "#         # force the last word to be a noun--not sure if this helps or not\n",
    "#         last_word_pos = result_tuples[-1][2][:2]\n",
    "#         if last_word_pos != \"NN\":\n",
    "#             result_tuples.pop()\n",
    "#             global pops\n",
    "#             pops +=1\n",
    "\n",
    "        # force the first word to not be a verb\n",
    "        first_word_pos = result_tuples[0][2][:2]\n",
    "        if first_word_pos == \"VB\":\n",
    "            result_tuples.pop()\n",
    "            global pops\n",
    "            pops +=1\n",
    "    \n",
    "       \n",
    "def main():\n",
    "    \n",
    "    text2tokens(text)\n",
    "    while len(result_tuples) < 7 and pops < max_pops:\n",
    "        get_word()\n",
    "        check_and_insert()\n",
    "        sequence_check()\n",
    "    \n",
    "main()   \n",
    "    \n",
    "    \n",
    "# print(result_tuples)\n",
    "\n",
    "result_words = []\n",
    "result_index = [] # result_index and result_pos lists are for easy reference to ensure the criteria worked\n",
    "result_pos = []\n",
    "for result in result_tuples:\n",
    "    result_words.append(result[0])\n",
    "    result_index.append(result[1])\n",
    "    result_pos.append(result[2])\n",
    "\n",
    "print(f\"\\n{result_words}\\n{result_index}\\n{result_pos}\\n\\npops={pops}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a048d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "import requests\n",
    "\n",
    "text = \"Cultivated peaches are divided into clingstones and freestones, depending on whether the flesh sticks to the stone or not; both can have either white or yellow flesh. Peaches with white flesh typically are very sweet with little acidity, while yellow-fleshed peaches typically have an acidic tang coupled with sweetness, though this also varies greatly. Both colors often have some red on their skins. Low-acid, white-fleshed peaches are the most popular kinds in China, Japan, and neighbouring Asian countries, while Europeans and North Americans have historically favoured the acidic, yellow-fleshed cultivars. Peach trees are relatively short-lived as compared with some other fruit trees. In some regions orchards are replanted after 8 to 10 years, while in others trees may produce satisfactorily for 20 to 25 years or more, depending upon their resistance to diseases, pests, and winter damage.\"\n",
    "\n",
    "highlight = ['Cultivated', 'clingstones', 'white', 'flesh', 'typically', 'yellow-fleshed']\n",
    "index = [0, 5, 29, 30, 31, 39]\n",
    "\n",
    "# customtkinter.set_appearance_mode('light')\n",
    "# customtkinter.set_default_color_theme(\"green\")\n",
    "\n",
    "poet = Tk()\n",
    "poet.geometry(\"600x500\")\n",
    "\n",
    "# textframe = Frame(master = poet, width = 300, height = 400)\n",
    "# textframe.place(relx=.5, rely=.5,anchor= 'center')\n",
    "\n",
    "poemtext = Text(poet, wrap = \"word\", spacing1 = 1, spacing2 = 2, spacing3 = 1, width = 40)\n",
    "poemtext.insert(INSERT,text)\n",
    "scrolly = Scrollbar(poemtext)\n",
    "poemtext.pack(anchor= 'center', expand = True)\n",
    "\n",
    "# poemtext.tag_add(\"help\",\"1.0\", \"1.4\")\n",
    "# poemtext.tag_config(\"help\", background = \"black\")\n",
    "\n",
    "\n",
    "\n",
    "for i in text:\n",
    "    if i not in highlight:\n",
    "        poemtext.tag_add(f\"{i}\",\"1.0\", \"1.26\")\n",
    "        poemtext.tag_config(f\"{i}\", background = \"black\")\n",
    "\n",
    "#we have a problem with the tag add: it takes the indexes as floats: the whole number is the word index and the decimal number is the char index. We need a way to convert our existing indices into this type        \n",
    "    \n",
    "poet.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90bc0e0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tkinter (from versions: none)\n",
      "ERROR: No matching distribution found for tkinter\n"
     ]
    }
   ],
   "source": [
    "pip install tkinter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9ae11",
   "metadata": {},
   "source": [
    "V3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0556f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nycta-p6\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nycta-p6\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nycta-p6\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\nycta-p6\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nycta-p6\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\nycta-p6\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NYCTA-P6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\NYCTA-P6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NYCTA-P6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "from tkinter import *\n",
    "\n",
    "\n",
    "# for reference:\n",
    "acronym_meanings = {\n",
    "    'CC': 'Coordinating conjunction',\n",
    "    'CD': 'Cardinal number',\n",
    "    'DT': 'Determiner',\n",
    "    'EX': 'Existential there',\n",
    "    'FW': 'Foreign word',\n",
    "    'IN': 'Preposition or subordinating conjunction',\n",
    "    'JJ*': 'Adjective',\n",
    "    'MD': 'Modal',\n",
    "    'NN*': 'Noun',\n",
    "    'PDT': 'Predeterminer',\n",
    "    'POS': 'Possessive ending',\n",
    "    'PRP*': 'Pronoun',\n",
    "    'RB*': 'Adverb',\n",
    "    'RP': 'Particle',\n",
    "    'SYM': 'Symbol',\n",
    "    'TO': 'To',\n",
    "    'UH': 'Interjection',\n",
    "    'VB*': 'Verb',\n",
    "    'WDT': 'Wh-determiner',\n",
    "    'WP*': 'Wh-pronoun',\n",
    "    'WRB': 'Wh-adverb'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a58633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a topic to make blackout poetry out of! jam\n",
      "\n",
      "Your search terms didn't match any results! Please try again\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "poet = Tk()\n",
    "poet.geometry(\"600x500\")\n",
    "\n",
    "\n",
    "def search_wiki():\n",
    "        global text\n",
    "        user_input = input(\"Choose a topic to make blackout poetry out of! \").lower()\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                text = wiki_pull(user_input)\n",
    "                if text == \"\":\n",
    "                    text = wiki_pull_backup(user_input)\n",
    "                    print(f\"Your search terms weren't a perfect match. Here's the closest article we found: {closest_match}\\n\")\n",
    "\n",
    "            except:\n",
    "                text = wiki_pull_backup(user_input)\n",
    "                print(f\"Your search terms weren't a perfect match. Here's the closest article we found: {closest_match}\\n\")\n",
    "        except:\n",
    "            print(f\"\\nYour search terms didn't match any results! Please try again\")\n",
    "            search_wiki()\n",
    "\n",
    "\n",
    "def wiki_pull(user_input):\n",
    "    wiki_api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # parameters\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": \"\", # get intro section only\n",
    "        \"explaintext\": \"\", # plain text\n",
    "        \"titles\": user_input}\n",
    "\n",
    "    # making request\n",
    "    response = requests.get(wiki_api_url, params=params)\n",
    "    response_json = response.json()\n",
    "\n",
    "    # have to get page_id so we can index the json dictionary for the extract\n",
    "    page_id = list(response_json[\"query\"][\"pages\"].keys())[0]\n",
    "    page_content = response_json[\"query\"][\"pages\"][page_id][\"extract\"]\n",
    "\n",
    "    return page_content\n",
    "\n",
    "\n",
    "\n",
    "def wiki_pull_backup(user_topic):\n",
    "    \n",
    "    wiki_api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # using search action instead of query\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": user_topic,\n",
    "        \"srprop\": \"\",\n",
    "        \"utf8\": \"\"}\n",
    "\n",
    "    # making request\n",
    "    response = requests.get(wiki_api_url, params=params)\n",
    "    response_json = response.json()\n",
    "\n",
    "    # selecting the page with closest match\n",
    "    global closest_match\n",
    "    closest_match = response_json[\"query\"][\"search\"][0][\"title\"]\n",
    "\n",
    "    # using the selected title for content request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": \"\", # get intro section only\n",
    "        \"explaintext\": \"\", # plain text\n",
    "        \"titles\": closest_match}\n",
    "\n",
    "    # making request for content\n",
    "    response = requests.get(wiki_api_url, params=params)\n",
    "    response_json = response.json()\n",
    "\n",
    "    # have to get page_id so we can index the json dictionary for the extract\n",
    "    page_id = list(response_json[\"query\"][\"pages\"].keys())[0]\n",
    "    page_content = response_json[\"query\"][\"pages\"][page_id][\"extract\"]\n",
    "\n",
    "    return page_content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the parts of speech that are allowed in the output\n",
    "allowed_pos = [\"JJ\", \"RB\", \"VB\", \"NN\"]\n",
    "\n",
    "\n",
    "result_tuples = []\n",
    "chosen = []\n",
    "\n",
    "# pops is used to limit the number of loops. Sometimes it bugs out if max_pops is set too high.\n",
    "max_pops = 25\n",
    "global pops\n",
    "pops = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def text2tokens(text):\n",
    "    global tokens, text_copy, text_offset\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # to preserve a version of the original text\n",
    "    text_copy = text\n",
    "    \n",
    "    # fixing \\n offset\n",
    "    text_offset = text.replace(\"\\n\", \" \\n\")\n",
    "    \n",
    "    # attaching the index from the tokenized text AND the original text to each word (both are later added to result_tuple)\n",
    "    # note: i think \\n's in the text source offset the text_index by 1 character\n",
    "    global both_indices\n",
    "    both_indices = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        text_index = text_copy.find(token) # searches for substring in string\n",
    "        text_index_end = text_index + (len(token)-1)\n",
    "        text_copy = text_copy.replace(token, \"x\" * len(token), 1) # replace the current token with all x's\n",
    "        both_indices.append((token, i, text_index, text_index_end))\n",
    "    \n",
    "    \n",
    "\n",
    "# not sure if stopwords are good or bad--a lot are already avoided due to \"allowed_pos\"\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     bad_tokens = []\n",
    "#     for word in bad_tokens:\n",
    "#         if word.lower() not in stop_words:\n",
    "#             bad_tokens.append(word)\n",
    "\n",
    "\n",
    "\n",
    "def get_word():\n",
    "    global choice, new_word, new_word_index, new_word_pos\n",
    "    choice = random.choice(range(len(tokens)))\n",
    "    chosen.append(choice)\n",
    "    new_word = tokens[choice]\n",
    "    new_word_index = choice\n",
    "    new_word_pos = pos_tag([new_word])[0][1][:2] # syntax: pos_tag generates a list of a tuple. This grabs the tuple, then grabs the 2nd element which is the POS as a string, then grabs the first 2 letters.\n",
    "    \n",
    "    \n",
    "    \n",
    "# this function is generalizable to any POS sequences i believe\n",
    "def sequence_check():\n",
    "    \"\"\"Checks to see if there is an sequence like 'adjective then noun' around the chosen word. \n",
    "    If so, then it adds both words to the results list.\"\"\"\n",
    "    \n",
    "    global new_word, new_word_index, new_word_pos\n",
    "    if choice > 0 and choice < len(tokens) - 1: # avoids list index out of range errors\n",
    "        prior_word = tokens[choice-1]\n",
    "        prior_word_index = choice-1\n",
    "        prior_word_pos = pos_tag([prior_word])[0][1][:2]\n",
    "\n",
    "        next_word = tokens[choice+1]\n",
    "        next_word_index = choice+1\n",
    "        next_word_pos = pos_tag([next_word])[0][1][:2]\n",
    "    \n",
    "    # adj-noun-verb\n",
    "        if prior_word_pos == \"JJ\" and new_word_pos == \"NN\" and next_word_pos ==\"VB\":\n",
    "            new_word = prior_word\n",
    "            new_word_index = prior_word_index\n",
    "            new_word_pos = prior_word_pos\n",
    "            check_and_insert()\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "            print(\"adj-noun-verb worked :)\")\n",
    "    \n",
    "    # adj-noun\n",
    "        elif prior_word_pos == \"JJ\" and new_word_pos == \"NN\":\n",
    "            new_word = prior_word\n",
    "            new_word_index = prior_word_index\n",
    "            new_word_pos = prior_word_pos\n",
    "            check_and_insert()\n",
    "            print(\"adj-noun worked :)\")\n",
    "\n",
    "        elif new_word_pos == \"JJ\" and next_word_pos == \"NN\":\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "            print(\"adj-noun worked :)\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def check_and_insert():\n",
    "    # Criteria:\n",
    "\n",
    "    # regular expression that matches only word characters. Prevents stuff like \"]\" which is a noun for some reason.\n",
    "    pattern = re.compile(r'\\w+')\n",
    "    if pattern.match(new_word):\n",
    "    \n",
    "    \n",
    "    \n",
    "        if new_word_pos in allowed_pos:\n",
    "\n",
    "            # find the index to insert the new word at.\n",
    "            index_to_insert = len(result_tuples)\n",
    "            for i, (word, index, pos, text_index, text_index_end) in enumerate(result_tuples): # this uses tuple unpacking and the enumerate function\n",
    "                if new_word_index < index:\n",
    "                    index_to_insert = i\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "            # check if the new_word_pos is different from the POS of the words before and after the insertion point. checks 2 things:\n",
    "                # 1. does the word before the insertion point have a different POS, or is the new word gonna be the first word in the list.\n",
    "                # 2. does the word after the insertion point have a different POS, or is the new word gonna be the last word in the list.\n",
    "            if (index_to_insert == 0 or result_tuples[index_to_insert-1][2] != new_word_pos) and (index_to_insert == len(result_tuples) or result_tuples[index_to_insert][2] != new_word_pos):\n",
    "                \n",
    "                # getting index from original text\n",
    "                for (token, token_index, text_index, text_index_end) in both_indices:\n",
    "                    if new_word_index == token_index:\n",
    "                        new_text_index = text_index\n",
    "                        new_text_index_end = text_index_end\n",
    "                \n",
    "                # insert the new word's tuple into the result_tuples list\n",
    "                result_tuples.insert(index_to_insert, (new_word, new_word_index, new_word_pos, new_text_index, new_text_index_end))\n",
    "\n",
    "\n",
    "        if len(result_tuples) == 7:\n",
    "\n",
    "    #         # force the last word to be a noun--not sure if this helps or not\n",
    "    #         last_word_pos = result_tuples[-1][2][:2]\n",
    "    #         if last_word_pos != \"NN\":\n",
    "    #             result_tuples.pop()\n",
    "    #             global pops\n",
    "    #             pops +=1\n",
    "\n",
    "            # force the first word to not be a verb\n",
    "            first_word_pos = result_tuples[0][2][:2]\n",
    "            if first_word_pos == \"VB\":\n",
    "                result_tuples.pop()\n",
    "                global pops\n",
    "                pops +=1\n",
    "    \n",
    "\n",
    "poemtext = Text(poet, wrap = \"word\", spacing1 = 1, spacing2 = 2, spacing3 = 1, width = 40)    \n",
    "    \n",
    "    \n",
    "while_counter = 0    \n",
    "def main():\n",
    "    search_wiki()\n",
    "    \n",
    "    text2tokens(text)\n",
    "    \n",
    "    poemtext.insert(INSERT,text)\n",
    "    poemtext.config(state = DISABLED)\n",
    "    scrolly = Scrollbar(poemtext)\n",
    "    poemtext.pack(anchor= 'center', expand = True)\n",
    "    \n",
    "    while_counter = 0\n",
    "    while len(result_tuples) < 7 and pops < max_pops and while_counter < 150:\n",
    "        get_word()\n",
    "        check_and_insert()\n",
    "        sequence_check()\n",
    "        \n",
    "        while_counter +=1\n",
    "        \n",
    "    \n",
    "    \n",
    "#\n",
    "    \n",
    "\n",
    "result_words = []\n",
    "result_index = [] # result_index and result_pos lists are for easy reference to ensure the criteria worked\n",
    "result_pos = []\n",
    "result_text_index = []\n",
    "for (word, index, pos, text_index, text_index_end) in result_tuples:\n",
    "    result_words.append(word)\n",
    "    result_index.append(index)\n",
    "    result_pos.append(pos)\n",
    "    result_text_index.append((text_index, text_index_end))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#Button for Refresh\n",
    "refresh = Button(poet,text = \"Refresh\", command = main)\n",
    "refresh.pack(anchor = \"w\")\n",
    "\n",
    "#Button to cause the blackout?\n",
    "\n",
    "\n",
    "#poemtext = Text(poet, wrap = \"word\", spacing1 = 1, spacing2 = 2, spacing3 = 1, width = 40)\n",
    "# poemtext.insert(INSERT,text)\n",
    "# poemtext.config(state = DISABLED)\n",
    "# scrolly = Scrollbar(poemtext)\n",
    "# poemtext.pack(anchor= 'center', expand = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(f\"\\n{result_words}\\n{result_index}\\n{result_pos}\\n{result_text_index}\\n\\npops={pops}\\n\\n{text_offset}\\n\")\n",
    "\n",
    "poet.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae56ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224877d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
